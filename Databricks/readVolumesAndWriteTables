# Read files from Databricks external volumes location and create a table if not exist. Replace table for new data uploads.

# Import required libraries.
from ..utils import utils
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("Spark SQL Example") \
    .getOrCreate()

def process_files(folder):
    # Define the custom delimiter
    delimiter = ";"
    # Define the header option
    header = True

    # Define the schema if needed
    # schema = StructType([
    #     StructField("col1", StringType(), True),
    #     StructField("col2", StringType(), True)
    # ])

    # Retrieve the list of files in the specified folder.
    files = dbutils.fs.ls(folder)

    for file in files:
        # Check if the file is a CSV file
        if file.name == "<fileName>.csv":
            # Create the URL using the variables
            url = f"/Volumes/<catalog>/<myFolder>/<mySubfolder>/{file.name}"

            # Read the CSV file with specified options
            df = spark.read \
                .option("delimiter", delimiter) \
                .option("header", header) \
                .csv(url)
              # Additional transformations to dataframe if needed. Drop column from the DataFrame.
            df = df.drop("colName")

            return df


# Create temp table of the function and rename
folder = "/Volumes/<catalog>/<folder>/<subfolder>"
df_read = process_files(folder)
df_read.createOrReplaceTempView("temp")


# Define catalog and schema to use
spark.sql("USE CATALOG myCatalog")
spark.sql("USE SCHEMA mySchema")


# Create external table from the temp table if not exist.
spark.sql("CREATE TABLE IF NOT EXISTS myTable LOCATION 'abfss://<containerName>@<storageName>.dfs.core.windows.net/<myFolder>/<myFolder>/<myTable>' AS SELECT * FROM temp")


# If there is new data in the source file then this replaces the existing data in the target delta table (if not merge).
# External table
spark.sql(
    "REPLACE TABLE myTable USING DELTA LOCATION 'abfss://<containerName>@<storageName>.dfs.core.windows.net/<myFolder>/<myFolder>/<myTable>' AS SELECT * FROM temp")


# Stop the Spark session
spark.stop()